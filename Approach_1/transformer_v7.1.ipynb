{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install epitran\n",
    "# !pip install torchtext\n",
    "# !pip install pytorch_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import pytorch_transformers as pt\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer, dtype_to_attr, is_tokenizer_serializable\n",
    "from collections import Counter, OrderedDict\n",
    "import torchtext\n",
    "from torchtext.vocab import Vocab\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_transformers as pt\n",
    "from pytorch_transformers.convert_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n",
    "import ast\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import epitran\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPT():\n",
    "    SGDR=True\n",
    "    epochs=25\n",
    "    d_model=960\n",
    "    n_layers = 6\n",
    "    heads=8\n",
    "    dropout=0.1\n",
    "    lr=0.0001\n",
    "    BATCH_SIZE = 3\n",
    "    CHAR_LSTM_EMB_DIM = 32\n",
    "    CHAR_LSTM_HID_DIM = 64\n",
    "    METRE_EMB_DIM = 32\n",
    "    SYLL_DIM = 32\n",
    "    src_pad = 0 #\"[PAD]\"\n",
    "    trg_pad = 0 #\"[PAD]\"\n",
    "    device_name = \"cpu\"\n",
    "    clip = 0.8\n",
    "    dataset_path = \"../rupo/\"\n",
    "    train_dataset_name = \"train_fin_v7.csv\"\n",
    "    test_dataset_name = \"test_fin_v7.csv\"\n",
    "    verses_dataset_name = \"train_fin_v8_man_rem_unk.csv\"\n",
    "    gen_loss_scale = 0.75\n",
    "    rhymes_loss_scale = 0.25\n",
    "    max_len = 510\n",
    "    \n",
    "opt = OPT()\n",
    "opt.device = torch.device(opt.device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /Users/michaelsolonin/.cache/torch/pytorch_transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /Users/michaelsolonin/.cache/torch/pytorch_transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "WARNING:pytorch_transformers.tokenization_bert:The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /Users/michaelsolonin/.cache/torch/pytorch_transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-multilingual-cased'\n",
    "bert = pt.BertModel.from_pretrained(model_name).eval()\n",
    "tokenizer = pt.BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(\"./data/train_fin_v7.csv\", names = list(\"abcdefghijkl\"))\n",
    "# verses = train[train.e != \"translation\"]\n",
    "# translation_part = train[train.e == \"translation\"].iloc[:50]\n",
    "# df = pd.concat([verses, translation_part])\n",
    "# df = df.sample(frac=1)\n",
    "# df.to_csv(\"./data/train_VERS_v7.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Char_LSTM(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Char_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=opt.CHAR_LSTM_EMB_DIM, \n",
    "            hidden_size=opt.CHAR_LSTM_HID_DIM, \n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.opt = opt\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        h0 = torch.zeros(2, seq_len, self.opt.CHAR_LSTM_HID_DIM).to(opt.device)\n",
    "        c0 = torch.zeros(2, seq_len, self.opt.CHAR_LSTM_HID_DIM).to(opt.device)\n",
    "        accum = torch.zeros([batch_size, seq_len, 2*self.opt.CHAR_LSTM_HID_DIM]).to(opt.device)\n",
    "        cur_batch = 0\n",
    "        for sent in x:\n",
    "            output , _ = self.lstm(sent, (h0, c0))\n",
    "            rnn_min = output.min(dim = 1).values\n",
    "            rnn_mean = output.mean(dim = 1)\n",
    "            rnn_max = output.max(dim = 1).values\n",
    "            to_average = torch.stack([rnn_min, rnn_mean, rnn_max],dim=0)\n",
    "            average = to_average.mean(dim=0)\n",
    "            accum[cur_batch] = average\n",
    "            cur_batch += 1\n",
    "        return accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 512, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model).to(opt.device)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, opt):\n",
    "    np_mask = np.triu(np.ones((1, size, size)),\n",
    "    k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    if opt.device == 0:\n",
    "        np_mask = np_mask.to(opt.device)\n",
    "    return np_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src, trg, opt):\n",
    "    \n",
    "    src_mask = (src != opt.src_pad).unsqueeze(-2).to(opt.device)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2).to(opt.device)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, opt).to(opt.device)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_sent = lambda x: x.split(' ')\n",
    "\n",
    "IDX = torchtext.data.Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "EB = torchtext.data.Field(sequential=False, use_vocab=False, preprocessing=lambda x: float(x),\n",
    "                          dtype=torch.float, batch_first=True)\n",
    "BERT_TXT = torchtext.data.Field(tokenize_sent, batch_first=True, \n",
    "                                pad_token=\"[PAD]\", unk_token=\"[UNK]\", \n",
    "                                init_token=\"[CLS]\", eos_token=\"[SEP]\")\n",
    "\n",
    "BERT_RU = torchtext.data.Field(tokenize_sent, batch_first=True, \n",
    "                                pad_token=\"[PAD]\", unk_token=\"[UNK]\", \n",
    "                                init_token=\"[CLS]\", eos_token=\"[SEP]\")\n",
    "\n",
    "METRE = torchtext.data.Field(tokenize=None, sequential=False, batch_first=True)\n",
    "\n",
    "SYLL = torchtext.data.Field(sequential=False, use_vocab=False, \n",
    "                            preprocessing=lambda x: list(map(float,ast.literal_eval(x))),\n",
    "                            dtype=torch.float, batch_first=True)\n",
    "char_nesting = torchtext.data.Field(tokenize=lambda x: list(x), \n",
    "                                    pad_token=\"<pad>\", unk_token=\"<unk>\",\n",
    "                                    init_token=\"<sos>\", eos_token=\"<eos>\", fix_length=16, \n",
    "                                    batch_first=True)\n",
    "XSAMPA = torchtext.data.NestedField(char_nesting, tokenize=lambda x: x.split(\" \"),\n",
    "                                    init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "RHYME = torchtext.data.Field(sequential=False, use_vocab=False, fix_length=100,\n",
    "                            preprocessing=lambda x: ast.literal_eval(x),\n",
    "                            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    ('IDX', IDX), ('EB', EB), ('RU_SYLL', SYLL), \n",
    "    ('EN_SYLL', SYLL), ('RU_METRE', METRE), ('EN_METRE', METRE), \n",
    "    ('EN_BERT', BERT_TXT), ('EN_XSAMPA', XSAMPA), ('RU_BERT', BERT_RU), \n",
    "    ('RU_XSAMPA', XSAMPA), (\"EN_RHYME\", RHYME), (\"RU_RHYME\", RHYME)\n",
    "]\n",
    "\n",
    "dataset_path = opt.dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_dataset = torchtext.data.TabularDataset.splits(\n",
    "    path=dataset_path, train=opt.verses_dataset_name,\n",
    "    format=\"csv\", fields=fields,\n",
    ")[0]\n",
    "\n",
    "train_dataset = torchtext.data.TabularDataset.splits(\n",
    "    path=dataset_path, train=opt.train_dataset_name,\n",
    "    format=\"csv\", fields=fields,\n",
    ")[0]\n",
    "\n",
    "test_dataset = torchtext.data.TabularDataset.splits(\n",
    "    path=dataset_path, train=opt.test_dataset_name,\n",
    "    format=\"csv\", fields=fields,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/119547\r",
      "100/119547\r",
      "200/119547\r",
      "300/119547\r",
      "400/119547\r",
      "500/119547\r",
      "600/119547\r",
      "700/119547\r",
      "800/119547\r",
      "900/119547\r",
      "1000/119547\r",
      "1100/119547\r",
      "1200/119547\r",
      "1300/119547\r",
      "1400/119547\r",
      "1500/119547\r",
      "1600/119547\r",
      "1700/119547\r",
      "1800/119547\r",
      "1900/119547\r",
      "2000/119547\r",
      "2100/119547\r",
      "2200/119547\r",
      "2300/119547\r",
      "2400/119547\r",
      "2500/119547\r",
      "2600/119547\r",
      "2700/119547\r",
      "2800/119547\r",
      "2900/119547\r",
      "3000/119547\r",
      "3100/119547\r",
      "3200/119547\r",
      "3300/119547\r",
      "3400/119547\r",
      "3500/119547\r",
      "3600/119547\r",
      "3700/119547\r",
      "3800/119547\r",
      "3900/119547\r",
      "4000/119547\r",
      "4100/119547\r",
      "4200/119547\r",
      "4300/119547\r",
      "4400/119547\r",
      "4500/119547\r",
      "4600/119547\r",
      "4700/119547\r",
      "4800/119547\r",
      "4900/119547\r",
      "5000/119547\r",
      "5100/119547\r",
      "5200/119547\r",
      "5300/119547\r",
      "5400/119547\r",
      "5500/119547\r",
      "5600/119547\r",
      "5700/119547\r",
      "5800/119547\r",
      "5900/119547\r",
      "6000/119547\r",
      "6100/119547\r",
      "6200/119547\r",
      "6300/119547\r",
      "6400/119547\r",
      "6500/119547\r",
      "6600/119547\r",
      "6700/119547\r",
      "6800/119547\r",
      "6900/119547\r",
      "7000/119547\r",
      "7100/119547\r",
      "7200/119547\r",
      "7300/119547\r",
      "7400/119547\r",
      "7500/119547\r",
      "7600/119547\r",
      "7700/119547\r",
      "7800/119547\r",
      "7900/119547\r",
      "8000/119547\r",
      "8100/119547\r",
      "8200/119547\r",
      "8300/119547\r",
      "8400/119547\r",
      "8500/119547\r",
      "8600/119547\r",
      "8700/119547\r",
      "8800/119547\r",
      "8900/119547\r",
      "9000/119547\r",
      "9100/119547\r",
      "9200/119547\r",
      "9300/119547\r",
      "9400/119547\r",
      "9500/119547\r",
      "9600/119547\r",
      "9700/119547\r",
      "9800/119547\r",
      "9900/119547\r",
      "10000/119547\r",
      "10100/119547\r",
      "10200/119547\r",
      "10300/119547\r",
      "10400/119547\r",
      "10500/119547\r",
      "10600/119547\r",
      "10700/119547\r",
      "10800/119547\r",
      "10900/119547\r",
      "11000/119547\r",
      "11100/119547\r",
      "11200/119547\r",
      "11300/119547\r",
      "11400/119547\r",
      "11500/119547\r",
      "11600/119547\r",
      "11700/119547\r",
      "11800/119547\r",
      "11900/119547\r",
      "12000/119547\r",
      "12100/119547\r",
      "12200/119547\r",
      "12300/119547\r",
      "12400/119547\r",
      "12500/119547\r",
      "12600/119547\r",
      "12700/119547\r",
      "12800/119547\r",
      "12900/119547\r",
      "13000/119547\r",
      "13100/119547\r",
      "13200/119547\r",
      "13300/119547\r",
      "13400/119547\r",
      "13500/119547\r",
      "13600/119547\r",
      "13700/119547\r",
      "13800/119547\r",
      "13900/119547\r",
      "14000/119547\r",
      "14100/119547\r",
      "14200/119547\r",
      "14300/119547\r",
      "14400/119547\r",
      "14500/119547\r",
      "14600/119547\r",
      "14700/119547\r",
      "14800/119547\r",
      "14900/119547\r",
      "15000/119547\r",
      "15100/119547\r",
      "15200/119547\r",
      "15300/119547\r",
      "15400/119547\r",
      "15500/119547\r",
      "15600/119547\r",
      "15700/119547\r",
      "15800/119547\r",
      "15900/119547\r",
      "16000/119547\r",
      "16100/119547\r",
      "16200/119547\r",
      "16300/119547\r",
      "16400/119547\r",
      "16500/119547\r",
      "16600/119547\r",
      "16700/119547\r",
      "16800/119547\r",
      "16900/119547\r",
      "17000/119547\r",
      "17100/119547\r",
      "17200/119547\r",
      "17300/119547\r",
      "17400/119547\r",
      "17500/119547\r",
      "17600/119547\r",
      "17700/119547\r",
      "17800/119547\r",
      "17900/119547\r",
      "18000/119547\r",
      "18100/119547\r",
      "18200/119547\r",
      "18300/119547\r",
      "18400/119547\r",
      "18500/119547\r",
      "18600/119547\r",
      "18700/119547\r",
      "18800/119547\r",
      "18900/119547\r",
      "19000/119547\r",
      "19100/119547\r",
      "19200/119547\r",
      "19300/119547\r",
      "19400/119547\r",
      "19500/119547\r",
      "19600/119547\r",
      "19700/119547\r",
      "19800/119547\r",
      "19900/119547\r",
      "20000/119547\r",
      "20100/119547\r",
      "20200/119547\r",
      "20300/119547\r",
      "20400/119547\r",
      "20500/119547\r",
      "20600/119547\r",
      "20700/119547\r",
      "20800/119547\r",
      "20900/119547\r",
      "21000/119547\r",
      "21100/119547\r",
      "21200/119547\r",
      "21300/119547\r",
      "21400/119547\r",
      "21500/119547\r",
      "21600/119547\r",
      "21700/119547\r",
      "21800/119547\r",
      "21900/119547\r",
      "22000/119547\r",
      "22100/119547\r",
      "22200/119547\r",
      "22300/119547\r",
      "22400/119547\r",
      "22500/119547\r",
      "22600/119547\r",
      "22700/119547\r",
      "22800/119547\r",
      "22900/119547\r",
      "23000/119547\r",
      "23100/119547\r",
      "23200/119547\r",
      "23300/119547\r",
      "23400/119547\r",
      "23500/119547\r",
      "23600/119547\r",
      "23700/119547\r",
      "23800/119547\r",
      "23900/119547\r",
      "24000/119547\r",
      "24100/119547\r",
      "24200/119547\r",
      "24300/119547\r",
      "24400/119547\r",
      "24500/119547\r",
      "24600/119547\r",
      "24700/119547\r",
      "24800/119547\r",
      "24900/119547\r",
      "25000/119547\r",
      "25100/119547\r",
      "25200/119547\r",
      "25300/119547\r",
      "25400/119547\r",
      "25500/119547\r",
      "25600/119547\r",
      "25700/119547\r",
      "25800/119547\r",
      "25900/119547\r",
      "26000/119547\r",
      "26100/119547\r",
      "26200/119547\r",
      "26300/119547\r",
      "26400/119547\r",
      "26500/119547\r",
      "26600/119547\r",
      "26700/119547\r",
      "26800/119547\r",
      "26900/119547\r",
      "27000/119547\r",
      "27100/119547\r",
      "27200/119547\r",
      "27300/119547\r",
      "27400/119547\r",
      "27500/119547\r",
      "27600/119547\r",
      "27700/119547\r",
      "27800/119547\r",
      "27900/119547\r",
      "28000/119547\r",
      "28100/119547\r",
      "28200/119547\r",
      "28300/119547\r",
      "28400/119547\r",
      "28500/119547\r",
      "28600/119547\r",
      "28700/119547\r",
      "28800/119547\r",
      "28900/119547\r",
      "29000/119547\r",
      "29100/119547\r",
      "29200/119547\r",
      "29300/119547\r",
      "29400/119547\r",
      "29500/119547\r",
      "29600/119547\r",
      "29700/119547\r",
      "29800/119547\r",
      "29900/119547\r",
      "30000/119547\r",
      "30100/119547\r",
      "30200/119547\r",
      "30300/119547\r",
      "30400/119547\r",
      "30500/119547\r",
      "30600/119547\r",
      "30700/119547\r",
      "30800/119547\r",
      "30900/119547\r",
      "31000/119547\r",
      "31100/119547\r",
      "31200/119547\r",
      "31300/119547\r",
      "31400/119547\r",
      "31500/119547\r",
      "31600/119547\r",
      "31700/119547\r",
      "31800/119547\r",
      "31900/119547\r",
      "32000/119547\r",
      "32100/119547\r",
      "32200/119547\r",
      "32300/119547\r",
      "32400/119547\r",
      "32500/119547\r",
      "32600/119547\r",
      "32700/119547\r",
      "32800/119547\r",
      "32900/119547\r",
      "33000/119547\r",
      "33100/119547\r",
      "33200/119547\r",
      "33300/119547\r",
      "33400/119547\r",
      "33500/119547\r",
      "33600/119547\r",
      "33700/119547\r",
      "33800/119547\r",
      "33900/119547\r",
      "34000/119547\r",
      "34100/119547\r",
      "34200/119547\r",
      "34300/119547\r",
      "34400/119547\r",
      "34500/119547\r",
      "34600/119547\r",
      "34700/119547\r",
      "34800/119547\r",
      "34900/119547\r",
      "35000/119547\r",
      "35100/119547\r",
      "35200/119547\r",
      "35300/119547\r",
      "35400/119547\r",
      "35500/119547\r",
      "35600/119547\r",
      "35700/119547\r",
      "35800/119547\r",
      "35900/119547\r",
      "36000/119547\r",
      "36100/119547\r",
      "36200/119547\r",
      "36300/119547\r",
      "36400/119547\r",
      "36500/119547\r",
      "36600/119547\r",
      "36700/119547\r",
      "36800/119547\r",
      "36900/119547\r",
      "37000/119547\r",
      "37100/119547\r",
      "37200/119547\r",
      "37300/119547\r",
      "37400/119547\r",
      "37500/119547\r",
      "37600/119547\r",
      "37700/119547\r",
      "37800/119547\r",
      "37900/119547\r",
      "38000/119547\r",
      "38100/119547\r",
      "38200/119547\r",
      "38300/119547\r",
      "38400/119547\r",
      "38500/119547\r",
      "38600/119547\r",
      "38700/119547\r",
      "38800/119547\r",
      "38900/119547\r",
      "39000/119547\r",
      "39100/119547\r",
      "39200/119547\r",
      "39300/119547\r",
      "39400/119547\r",
      "39500/119547\r",
      "39600/119547\r",
      "39700/119547\r",
      "39800/119547\r",
      "39900/119547\r",
      "40000/119547\r",
      "40100/119547\r",
      "40200/119547\r",
      "40300/119547\r",
      "40400/119547\r",
      "40500/119547\r",
      "40600/119547\r",
      "40700/119547\r",
      "40800/119547\r",
      "40900/119547\r",
      "41000/119547\r",
      "41100/119547\r",
      "41200/119547\r",
      "41300/119547\r",
      "41400/119547\r",
      "41500/119547\r",
      "41600/119547\r",
      "41700/119547\r",
      "41800/119547\r",
      "41900/119547\r",
      "42000/119547\r",
      "42100/119547\r",
      "42200/119547\r",
      "42300/119547\r",
      "42400/119547\r",
      "42500/119547\r",
      "42600/119547\r",
      "42700/119547\r",
      "42800/119547\r",
      "42900/119547\r",
      "43000/119547\r",
      "43100/119547\r",
      "43200/119547\r",
      "43300/119547\r",
      "43400/119547\r",
      "43500/119547\r",
      "43600/119547\r",
      "43700/119547\r",
      "43800/119547\r",
      "43900/119547\r",
      "44000/119547\r",
      "44100/119547\r",
      "44200/119547\r",
      "44300/119547\r",
      "44400/119547\r",
      "44500/119547\r",
      "44600/119547\r",
      "44700/119547\r",
      "44800/119547\r",
      "44900/119547\r",
      "45000/119547\r",
      "45100/119547\r",
      "45200/119547\r",
      "45300/119547\r",
      "45400/119547\r",
      "45500/119547\r",
      "45600/119547\r",
      "45700/119547\r",
      "45800/119547\r",
      "45900/119547\r",
      "46000/119547\r",
      "46100/119547\r",
      "46200/119547\r",
      "46300/119547\r",
      "46400/119547\r",
      "46500/119547\r",
      "46600/119547\r",
      "46700/119547\r",
      "46800/119547\r",
      "46900/119547\r",
      "47000/119547\r",
      "47100/119547\r",
      "47200/119547\r",
      "47300/119547\r",
      "47400/119547\r",
      "47500/119547\r",
      "47600/119547\r",
      "47700/119547\r",
      "47800/119547\r",
      "47900/119547\r",
      "48000/119547\r",
      "48100/119547\r",
      "48200/119547\r",
      "48300/119547\r",
      "48400/119547\r",
      "48500/119547\r",
      "48600/119547\r",
      "48700/119547\r",
      "48800/119547\r",
      "48900/119547\r",
      "49000/119547\r",
      "49100/119547\r",
      "49200/119547\r",
      "49300/119547\r",
      "49400/119547\r",
      "49500/119547\r",
      "49600/119547\r",
      "49700/119547\r",
      "49800/119547\r",
      "49900/119547\r",
      "50000/119547\r",
      "50100/119547\r",
      "50200/119547\r",
      "50300/119547\r",
      "50400/119547\r",
      "50500/119547\r",
      "50600/119547\r",
      "50700/119547\r",
      "50800/119547\r",
      "50900/119547\r",
      "51000/119547\r",
      "51100/119547\r",
      "51200/119547\r",
      "51300/119547\r",
      "51400/119547\r",
      "51500/119547\r",
      "51600/119547\r",
      "51700/119547\r",
      "51800/119547\r",
      "51900/119547\r",
      "52000/119547\r",
      "52100/119547\r",
      "52200/119547\r",
      "52300/119547\r",
      "52400/119547\r",
      "52500/119547\r",
      "52600/119547\r",
      "52700/119547\r",
      "52800/119547\r",
      "52900/119547\r",
      "53000/119547\r",
      "53100/119547\r",
      "53200/119547\r",
      "53300/119547\r",
      "53400/119547\r",
      "53500/119547\r",
      "53600/119547\r",
      "53700/119547\r",
      "53800/119547\r",
      "53900/119547\r",
      "54000/119547\r",
      "54100/119547\r",
      "54200/119547\r",
      "54300/119547\r",
      "54400/119547\r",
      "54500/119547\r",
      "54600/119547\r",
      "54700/119547\r",
      "54800/119547\r",
      "54900/119547\r",
      "55000/119547\r",
      "55100/119547\r",
      "55200/119547\r",
      "55300/119547\r",
      "55400/119547\r",
      "55500/119547\r",
      "55600/119547\r",
      "55700/119547\r",
      "55800/119547\r",
      "55900/119547\r",
      "56000/119547\r",
      "56100/119547\r",
      "56200/119547\r",
      "56300/119547\r",
      "56400/119547\r",
      "56500/119547\r",
      "56600/119547\r",
      "56700/119547\r",
      "56800/119547\r",
      "56900/119547\r",
      "57000/119547\r",
      "57100/119547\r",
      "57200/119547\r",
      "57300/119547\r",
      "57400/119547\r",
      "57500/119547\r",
      "57600/119547\r",
      "57700/119547\r",
      "57800/119547\r",
      "57900/119547\r",
      "58000/119547\r",
      "58100/119547\r",
      "58200/119547\r",
      "58300/119547\r",
      "58400/119547\r",
      "58500/119547\r",
      "58600/119547\r",
      "58700/119547\r",
      "58800/119547\r",
      "58900/119547\r",
      "59000/119547\r",
      "59100/119547\r",
      "59200/119547\r",
      "59300/119547\r",
      "59400/119547\r",
      "59500/119547\r",
      "59600/119547\r",
      "59700/119547\r",
      "59800/119547\r",
      "59900/119547\r",
      "60000/119547\r",
      "60100/119547\r",
      "60200/119547\r",
      "60300/119547\r",
      "60400/119547\r",
      "60500/119547\r",
      "60600/119547\r",
      "60700/119547\r",
      "60800/119547\r",
      "60900/119547\r",
      "61000/119547\r",
      "61100/119547\r",
      "61200/119547\r",
      "61300/119547\r",
      "61400/119547\r",
      "61500/119547\r",
      "61600/119547\r",
      "61700/119547\r",
      "61800/119547\r",
      "61900/119547\r",
      "62000/119547\r",
      "62100/119547\r",
      "62200/119547\r",
      "62300/119547\r",
      "62400/119547\r",
      "62500/119547\r",
      "62600/119547\r",
      "62700/119547\r",
      "62800/119547\r",
      "62900/119547\r",
      "63000/119547\r",
      "63100/119547\r",
      "63200/119547\r",
      "63300/119547\r",
      "63400/119547\r",
      "63500/119547\r",
      "63600/119547\r",
      "63700/119547\r",
      "63800/119547\r",
      "63900/119547\r",
      "64000/119547\r",
      "64100/119547\r",
      "64200/119547\r",
      "64300/119547\r",
      "64400/119547\r",
      "64500/119547\r",
      "64600/119547\r",
      "64700/119547\r",
      "64800/119547\r",
      "64900/119547\r",
      "65000/119547\r",
      "65100/119547\r",
      "65200/119547\r",
      "65300/119547\r",
      "65400/119547\r",
      "65500/119547\r",
      "65600/119547\r",
      "65700/119547\r",
      "65800/119547\r",
      "65900/119547\r",
      "66000/119547\r",
      "66100/119547\r",
      "66200/119547\r",
      "66300/119547\r",
      "66400/119547\r",
      "66500/119547\r",
      "66600/119547\r",
      "66700/119547\r",
      "66800/119547\r",
      "66900/119547\r",
      "67000/119547\r",
      "67100/119547\r",
      "67200/119547\r",
      "67300/119547\r",
      "67400/119547\r",
      "67500/119547\r",
      "67600/119547\r",
      "67700/119547\r",
      "67800/119547\r",
      "67900/119547\r",
      "68000/119547\r",
      "68100/119547\r",
      "68200/119547\r",
      "68300/119547\r",
      "68400/119547\r",
      "68500/119547\r",
      "68600/119547\r",
      "68700/119547\r",
      "68800/119547\r",
      "68900/119547\r",
      "69000/119547\r",
      "69100/119547\r",
      "69200/119547\r",
      "69300/119547\r",
      "69400/119547\r",
      "69500/119547\r",
      "69600/119547\r",
      "69700/119547\r",
      "69800/119547\r",
      "69900/119547\r",
      "70000/119547\r",
      "70100/119547\r",
      "70200/119547\r",
      "70300/119547\r",
      "70400/119547\r",
      "70500/119547\r",
      "70600/119547\r",
      "70700/119547\r",
      "70800/119547\r",
      "70900/119547\r",
      "71000/119547\r",
      "71100/119547\r",
      "71200/119547\r",
      "71300/119547\r",
      "71400/119547\r",
      "71500/119547\r",
      "71600/119547\r",
      "71700/119547\r",
      "71800/119547\r",
      "71900/119547\r",
      "72000/119547\r",
      "72100/119547\r",
      "72200/119547\r",
      "72300/119547\r",
      "72400/119547\r",
      "72500/119547\r",
      "72600/119547\r",
      "72700/119547\r",
      "72800/119547\r",
      "72900/119547\r",
      "73000/119547\r",
      "73100/119547\r",
      "73200/119547\r",
      "73300/119547\r",
      "73400/119547\r",
      "73500/119547\r",
      "73600/119547\r",
      "73700/119547\r",
      "73800/119547\r",
      "73900/119547\r",
      "74000/119547\r",
      "74100/119547\r",
      "74200/119547\r",
      "74300/119547\r",
      "74400/119547\r",
      "74500/119547\r",
      "74600/119547\r",
      "74700/119547\r",
      "74800/119547\r",
      "74900/119547\r",
      "75000/119547\r",
      "75100/119547\r",
      "75200/119547\r",
      "75300/119547\r",
      "75400/119547\r",
      "75500/119547\r",
      "75600/119547\r",
      "75700/119547\r",
      "75800/119547\r",
      "75900/119547\r",
      "76000/119547\r",
      "76100/119547\r",
      "76200/119547\r",
      "76300/119547\r",
      "76400/119547\r",
      "76500/119547\r",
      "76600/119547\r",
      "76700/119547\r",
      "76800/119547\r",
      "76900/119547\r",
      "77000/119547\r",
      "77100/119547\r",
      "77200/119547\r",
      "77300/119547\r",
      "77400/119547\r",
      "77500/119547\r",
      "77600/119547\r",
      "77700/119547\r",
      "77800/119547\r",
      "77900/119547\r",
      "78000/119547\r",
      "78100/119547\r",
      "78200/119547\r",
      "78300/119547\r",
      "78400/119547\r",
      "78500/119547\r",
      "78600/119547\r",
      "78700/119547\r",
      "78800/119547\r",
      "78900/119547\r",
      "79000/119547\r",
      "79100/119547\r",
      "79200/119547\r",
      "79300/119547\r",
      "79400/119547\r",
      "79500/119547\r",
      "79600/119547\r",
      "79700/119547\r",
      "79800/119547\r",
      "79900/119547\r",
      "80000/119547\r",
      "80100/119547\r",
      "80200/119547\r",
      "80300/119547\r",
      "80400/119547\r",
      "80500/119547\r",
      "80600/119547\r",
      "80700/119547\r",
      "80800/119547\r",
      "80900/119547\r",
      "81000/119547\r",
      "81100/119547\r",
      "81200/119547\r",
      "81300/119547\r",
      "81400/119547\r",
      "81500/119547\r",
      "81600/119547\r",
      "81700/119547\r",
      "81800/119547\r",
      "81900/119547\r",
      "82000/119547\r",
      "82100/119547\r",
      "82200/119547\r",
      "82300/119547\r",
      "82400/119547\r",
      "82500/119547\r",
      "82600/119547\r",
      "82700/119547\r",
      "82800/119547\r",
      "82900/119547\r",
      "83000/119547\r",
      "83100/119547\r",
      "83200/119547\r",
      "83300/119547\r",
      "83400/119547\r",
      "83500/119547\r",
      "83600/119547\r",
      "83700/119547\r",
      "83800/119547\r",
      "83900/119547\r",
      "84000/119547\r",
      "84100/119547\r",
      "84200/119547\r",
      "84300/119547\r",
      "84400/119547\r",
      "84500/119547\r",
      "84600/119547\r",
      "84700/119547\r",
      "84800/119547\r",
      "84900/119547\r",
      "85000/119547\r",
      "85100/119547\r",
      "85200/119547\r",
      "85300/119547\r",
      "85400/119547\r",
      "85500/119547\r",
      "85600/119547\r",
      "85700/119547\r",
      "85800/119547\r",
      "85900/119547\r",
      "86000/119547\r",
      "86100/119547\r",
      "86200/119547\r",
      "86300/119547\r",
      "86400/119547\r",
      "86500/119547\r",
      "86600/119547\r",
      "86700/119547\r",
      "86800/119547\r",
      "86900/119547\r",
      "87000/119547\r",
      "87100/119547\r",
      "87200/119547\r",
      "87300/119547\r",
      "87400/119547\r",
      "87500/119547\r",
      "87600/119547\r",
      "87700/119547\r",
      "87800/119547\r",
      "87900/119547\r",
      "88000/119547\r",
      "88100/119547\r",
      "88200/119547\r",
      "88300/119547\r",
      "88400/119547\r",
      "88500/119547\r",
      "88600/119547\r",
      "88700/119547\r",
      "88800/119547\r",
      "88900/119547\r",
      "89000/119547\r",
      "89100/119547\r",
      "89200/119547\r",
      "89300/119547\r",
      "89400/119547\r",
      "89500/119547\r",
      "89600/119547\r",
      "89700/119547\r",
      "89800/119547\r",
      "89900/119547\r",
      "90000/119547\r",
      "90100/119547\r",
      "90200/119547\r",
      "90300/119547\r",
      "90400/119547\r",
      "90500/119547\r",
      "90600/119547\r",
      "90700/119547\r",
      "90800/119547\r",
      "90900/119547\r",
      "91000/119547\r",
      "91100/119547\r",
      "91200/119547\r",
      "91300/119547\r",
      "91400/119547\r",
      "91500/119547\r",
      "91600/119547\r",
      "91700/119547\r",
      "91800/119547\r",
      "91900/119547\r",
      "92000/119547\r",
      "92100/119547\r",
      "92200/119547\r",
      "92300/119547\r",
      "92400/119547\r",
      "92500/119547\r",
      "92600/119547\r",
      "92700/119547\r",
      "92800/119547\r",
      "92900/119547\r",
      "93000/119547\r",
      "93100/119547\r",
      "93200/119547\r",
      "93300/119547\r",
      "93400/119547\r",
      "93500/119547\r",
      "93600/119547\r",
      "93700/119547\r",
      "93800/119547\r",
      "93900/119547\r",
      "94000/119547\r",
      "94100/119547\r",
      "94200/119547\r",
      "94300/119547\r",
      "94400/119547\r",
      "94500/119547\r",
      "94600/119547\r",
      "94700/119547\r",
      "94800/119547\r",
      "94900/119547\r",
      "95000/119547\r",
      "95100/119547\r",
      "95200/119547\r",
      "95300/119547\r",
      "95400/119547\r",
      "95500/119547\r",
      "95600/119547\r",
      "95700/119547\r",
      "95800/119547\r",
      "95900/119547\r",
      "96000/119547\r",
      "96100/119547\r",
      "96200/119547\r",
      "96300/119547\r",
      "96400/119547\r",
      "96500/119547\r",
      "96600/119547\r",
      "96700/119547\r",
      "96800/119547\r",
      "96900/119547\r",
      "97000/119547\r",
      "97100/119547\r",
      "97200/119547\r",
      "97300/119547\r",
      "97400/119547\r",
      "97500/119547\r",
      "97600/119547\r",
      "97700/119547\r",
      "97800/119547\r",
      "97900/119547\r",
      "98000/119547\r",
      "98100/119547\r",
      "98200/119547\r",
      "98300/119547\r",
      "98400/119547\r",
      "98500/119547\r",
      "98600/119547\r",
      "98700/119547\r",
      "98800/119547\r",
      "98900/119547\r",
      "99000/119547\r",
      "99100/119547\r",
      "99200/119547\r",
      "99300/119547\r",
      "99400/119547\r",
      "99500/119547\r",
      "99600/119547\r",
      "99700/119547\r",
      "99800/119547\r",
      "99900/119547\r",
      "100000/119547\r",
      "100100/119547\r",
      "100200/119547\r",
      "100300/119547\r",
      "100400/119547\r",
      "100500/119547\r",
      "100600/119547\r",
      "100700/119547\r",
      "100800/119547\r",
      "100900/119547\r",
      "101000/119547\r",
      "101100/119547\r",
      "101200/119547\r",
      "101300/119547\r",
      "101400/119547\r",
      "101500/119547\r",
      "101600/119547\r",
      "101700/119547\r",
      "101800/119547\r",
      "101900/119547\r",
      "102000/119547\r",
      "102100/119547\r",
      "102200/119547\r",
      "102300/119547\r",
      "102400/119547\r",
      "102500/119547\r",
      "102600/119547\r",
      "102700/119547\r",
      "102800/119547\r",
      "102900/119547\r",
      "103000/119547\r",
      "103100/119547\r",
      "103200/119547\r",
      "103300/119547\r",
      "103400/119547\r",
      "103500/119547\r",
      "103600/119547\r",
      "103700/119547\r",
      "103800/119547\r",
      "103900/119547\r",
      "104000/119547\r",
      "104100/119547\r",
      "104200/119547\r",
      "104300/119547\r",
      "104400/119547\r",
      "104500/119547\r",
      "104600/119547\r",
      "104700/119547\r",
      "104800/119547\r",
      "104900/119547\r",
      "105000/119547\r",
      "105100/119547\r",
      "105200/119547\r",
      "105300/119547\r",
      "105400/119547\r",
      "105500/119547\r",
      "105600/119547\r",
      "105700/119547\r",
      "105800/119547\r",
      "105900/119547\r",
      "106000/119547\r",
      "106100/119547\r",
      "106200/119547\r",
      "106300/119547\r",
      "106400/119547\r",
      "106500/119547\r",
      "106600/119547\r",
      "106700/119547\r",
      "106800/119547\r",
      "106900/119547\r",
      "107000/119547\r",
      "107100/119547\r",
      "107200/119547\r",
      "107300/119547\r",
      "107400/119547\r",
      "107500/119547\r",
      "107600/119547\r",
      "107700/119547\r",
      "107800/119547\r",
      "107900/119547\r",
      "108000/119547\r",
      "108100/119547\r",
      "108200/119547\r",
      "108300/119547\r",
      "108400/119547\r",
      "108500/119547\r",
      "108600/119547\r",
      "108700/119547\r",
      "108800/119547\r",
      "108900/119547\r",
      "109000/119547\r",
      "109100/119547\r",
      "109200/119547\r",
      "109300/119547\r",
      "109400/119547\r",
      "109500/119547\r",
      "109600/119547\r",
      "109700/119547\r",
      "109800/119547\r",
      "109900/119547\r",
      "110000/119547\r",
      "110100/119547\r",
      "110200/119547\r",
      "110300/119547\r",
      "110400/119547\r",
      "110500/119547\r",
      "110600/119547\r",
      "110700/119547\r",
      "110800/119547\r",
      "110900/119547\r",
      "111000/119547\r",
      "111100/119547\r",
      "111200/119547\r",
      "111300/119547\r",
      "111400/119547\r",
      "111500/119547\r",
      "111600/119547\r",
      "111700/119547\r",
      "111800/119547\r",
      "111900/119547\r",
      "112000/119547\r",
      "112100/119547\r",
      "112200/119547\r",
      "112300/119547\r",
      "112400/119547\r",
      "112500/119547\r",
      "112600/119547\r",
      "112700/119547\r",
      "112800/119547\r",
      "112900/119547\r",
      "113000/119547\r",
      "113100/119547\r",
      "113200/119547\r",
      "113300/119547\r",
      "113400/119547\r",
      "113500/119547\r",
      "113600/119547\r",
      "113700/119547\r",
      "113800/119547\r",
      "113900/119547\r",
      "114000/119547\r",
      "114100/119547\r",
      "114200/119547\r",
      "114300/119547\r",
      "114400/119547\r",
      "114500/119547\r",
      "114600/119547\r",
      "114700/119547\r",
      "114800/119547\r",
      "114900/119547\r",
      "115000/119547\r",
      "115100/119547\r",
      "115200/119547\r",
      "115300/119547\r",
      "115400/119547\r",
      "115500/119547\r",
      "115600/119547\r",
      "115700/119547\r",
      "115800/119547\r",
      "115900/119547\r",
      "116000/119547\r",
      "116100/119547\r",
      "116200/119547\r",
      "116300/119547\r",
      "116400/119547\r",
      "116500/119547\r",
      "116600/119547\r",
      "116700/119547\r",
      "116800/119547\r",
      "116900/119547\r",
      "117000/119547\r",
      "117100/119547\r",
      "117200/119547\r",
      "117300/119547\r",
      "117400/119547\r",
      "117500/119547\r",
      "117600/119547\r",
      "117700/119547\r",
      "117800/119547\r",
      "117900/119547\r",
      "118000/119547\r",
      "118100/119547\r",
      "118200/119547\r",
      "118300/119547\r",
      "118400/119547\r",
      "118500/119547\r",
      "118600/119547\r",
      "118700/119547\r",
      "118800/119547\r",
      "118900/119547\r",
      "119000/119547\r",
      "119100/119547\r",
      "119200/119547\r",
      "119300/119547\r",
      "119400/119547\r",
      "119500/119547\r"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "max_len = len(tokenizer.vocab) + 5\n",
    "for idx, tup in enumerate(list(tokenizer.vocab.items())):\n",
    "    if idx % 100 == 0: print(\"{}/{}\".format(idx, max_len-5), end = \"\\r\")\n",
    "    tok = tup[0]\n",
    "    counter[tok] = max_len - idx\n",
    "vocab_bert = torchtext.vocab.Vocab(counter,specials=[])\n",
    "\n",
    "BERT_TXT.build_vocab([None])\n",
    "BERT_TXT.vocab = vocab_bert\n",
    "BERT_RU.build_vocab(train_dataset)\n",
    "XSAMPA.build_vocab(train_dataset)\n",
    "METRE.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BERT_TXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9764"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BERT_RU.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = torchtext.data.BucketIterator(\n",
    "    verses_dataset, batch_size=opt.BATCH_SIZE, \n",
    "    device=opt.device, repeat=False, sort_key=lambda x: (len(x.EN_BERT) + len(x.RU_BERT)), \n",
    "    sort_within_batch=True, shuffle=True)\n",
    "\n",
    "test_iterator = torchtext.data.BucketIterator(\n",
    "    test_dataset, batch_size=1, \n",
    "    device=opt.device, repeat=False, sort_key=lambda x: (len(x.EN_BERT)), \n",
    "    sort_within_batch=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12636\r"
     ]
    }
   ],
   "source": [
    "zzz = 0\n",
    "for idx, batch in enumerate(train_iterator):\n",
    "    print(idx, end = \"\\r\")\n",
    "    zzz = batch.RU_BERT\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [1, 1, 1, 2, 3]\n",
    "# counter = Counter(a)\n",
    "# vocab = torchtext.vocab.Vocab(counter)\n",
    "# vocab.stoi[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12637"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder_en(nn.Module):\n",
    "    def __init__(self, opt, bert, char_LSTM, syll_net, emb_metre, emb_char, d_model=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.bert = bert\n",
    "        self.char_lstm = char_LSTM\n",
    "        self.syll_net = syll_net\n",
    "        self.emb_metre = emb_metre\n",
    "        self.emb_char = emb_char\n",
    "    def forward(self, x):\n",
    "        seq_len = batch.EN_BERT.shape[1]\n",
    "        embedded_chars = self.emb_char(x.EN_XSAMPA)\n",
    "        out_bert = self.bert(x.EN_BERT)[0]\n",
    "        out_char_lstm = self.char_lstm(embedded_chars)\n",
    "        out_syll = self.syll_net(x.EN_SYLL).unsqueeze(1).repeat([1,seq_len,1])\n",
    "        out_metre = self.emb_metre(x.EN_METRE).unsqueeze(1).repeat([1,seq_len,1])\n",
    "        \n",
    "        out = torch.cat([out_bert, out_char_lstm, out_syll, out_metre], dim = 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder_ru(nn.Module):\n",
    "    def __init__(self, opt, bert, char_LSTM, syll_net, emb_metre, emb_char, d_model=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.bert = bert\n",
    "        self.char_lstm = char_LSTM\n",
    "        self.syll_net = syll_net\n",
    "        self.emb_metre = emb_metre\n",
    "        self.emb_char = emb_char\n",
    "        self.emb_brt_like = nn.Embedding(len(BERT_RU.vocab), 768)\n",
    "    def forward(self, x, xsampa = None, syll = None, metre = None):\n",
    "        seq_len = batch.RU_BERT.shape[1]\n",
    "        embedded_chars = self.emb_char(x.RU_XSAMPA)\n",
    "#         out_bert = self.bert(x.RU_BERT)[0]\n",
    "        out_bert = self.emb_brt_like(x.RU_BERT)\n",
    "        out_char_lstm = self.char_lstm(embedded_chars)\n",
    "        out_syll = self.syll_net(x.RU_SYLL).unsqueeze(1).repeat([1,seq_len,1])\n",
    "        out_metre = self.emb_metre(x.RU_METRE).unsqueeze(1).repeat([1,seq_len,1])\n",
    "        \n",
    "        out = torch.cat([out_bert, out_char_lstm, out_syll, out_metre], dim = 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size).to(opt.device))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size).to(opt.device))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        # perform linear operation and split into N heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        # transpose to get dimensions bs * N * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, \\\n",
    "        src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt, vocab_size, d_model, N, heads, dropout, bert, \n",
    "                 char_lstm, syll_net, emb_metre, emb_char):\n",
    "        super().__init__()\n",
    "        self.N = 1 #N\n",
    "        self.embed = Embedder_en(opt, bert, char_lstm, syll_net,\n",
    "                     emb_metre, emb_char, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, opt, vocab_size, d_model, N, heads, dropout, bert, \n",
    "                 char_lstm, syll_net, emb_metre, emb_char):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder_ru(opt, bert, char_lstm, syll_net,\n",
    "                     emb_metre, emb_char, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "#         x = self.embed(trg)[:,:-1]\n",
    "        if self.training:\n",
    "            x = self.embed(trg)[:,:-1]\n",
    "        else: # trg is already embedded\n",
    "            x = trg\n",
    "#         print(x.shape, src_mask.shape, trg_mask.shape)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, opt, vocab_size, d_model, N, heads, dropout, bert, \n",
    "                 char_lstm, syll_net, emb_metre, emb_char):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(opt, vocab_size, d_model, N, heads, dropout, bert, \n",
    "                 char_lstm, syll_net, emb_metre, emb_char)\n",
    "        self.decoder = Decoder(opt, vocab_size, d_model, N, heads, dropout, bert, \n",
    "                 char_lstm, syll_net, emb_metre, emb_char)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(x, src_mask)\n",
    "        d_output = self.decoder(x, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output, e_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_rhymes(batch, preds):\n",
    "    batch_size = preds.shape[0]#.to(opt.device)\n",
    "    loss_rhymes = torch.tensor(0.0).to(opt.device)\n",
    "    trg = batch.RU_BERT.to(opt.device)\n",
    "    for c in range(batch_size):\n",
    "        cur_rhyme = batch.RU_RHYME.to(opt.device)\n",
    "        needed_ids = cur_rhyme[c][cur_rhyme[c] != -1]\n",
    "        p = preds[c, needed_ids, :]\n",
    "        tr = trg[:, 1:][c, needed_ids]\n",
    "        loss_rhymes += F.cross_entropy(p, tr).to(opt.device)\n",
    "    loss_rhymes /= batch_size\n",
    "    return loss_rhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine annealing with restarts.\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : torch.optim.Optimizer\n",
    "    T_max : int\n",
    "        The maximum number of iterations within the first cycle.\n",
    "\n",
    "    eta_min : float, optional (default: 0)\n",
    "        The minimum learning rate.\n",
    "\n",
    "    last_epoch : int, optional (default: -1)\n",
    "        The index of the last epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 T_max: int,\n",
    "                 eta_min: float = 0.,\n",
    "                 last_epoch: int = -1,\n",
    "                 factor: float = 1.) -> None:\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.factor = factor\n",
    "        self._last_restart: int = 0\n",
    "        self._cycle_counter: int = 0\n",
    "        self._cycle_factor: float = 1.\n",
    "        self._updated_cycle_len: int = T_max\n",
    "        self._initialized: bool = False\n",
    "        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Get updated learning rate.\"\"\"\n",
    "        if not self._initialized:\n",
    "            self._initialized = True\n",
    "            return self.base_lrs\n",
    "        step = self.last_epoch + 1\n",
    "        self._cycle_counter = step - self._last_restart\n",
    "        lrs = [\n",
    "            (\n",
    "                self.eta_min + ((lr - self.eta_min) / 2) *\n",
    "                (\n",
    "                    np.cos(\n",
    "                        np.pi *\n",
    "                        ((self._cycle_counter) % self._updated_cycle_len) /\n",
    "                        self._updated_cycle_len\n",
    "                    ) + 1\n",
    "                )\n",
    "            ) for lr in self.base_lrs\n",
    "        ]\n",
    "        if self._cycle_counter % self._updated_cycle_len == 0:\n",
    "            self._cycle_factor *= self.factor\n",
    "            self._cycle_counter = 0\n",
    "            self._updated_cycle_len = int(self._cycle_factor * self.T_max)\n",
    "            self._last_restart = step\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = 0\n",
    "def train_batch(opt, batch, model, optimizer, \n",
    "                pred_syll_layer, pred_metre_layer):\n",
    "    global preds\n",
    "    src = batch.EN_BERT.to(opt.device)\n",
    "    trg = batch.RU_BERT.to(opt.device)\n",
    "    trg_input = trg[:, :-1]\n",
    "    src_mask, trg_mask = create_masks(src, trg_input, opt)\n",
    "    \n",
    "    preds, e_outputs = model(batch, src_mask, trg_mask)\n",
    "    ys = trg[:, 1:].contiguous().view(-1)\n",
    "    \n",
    "    pred_syll = pred_syll_layer(e_outputs.max(dim = 1).values)\n",
    "    pred_metre = pred_metre_layer(e_outputs.max(dim = 1).values)\n",
    "    \n",
    "    opt.optimizer.zero_grad()\n",
    "    \n",
    "    loss_syll = F.mse_loss(pred_syll, syll_net(batch.RU_SYLL), reduction=\"mean\").to(opt.device)\n",
    "    loss_metre = F.mse_loss(pred_metre, emb_metre(batch.RU_METRE), reduction=\"mean\").to(opt.device)\n",
    "    loss_rhymes = get_loss_rhymes(batch, preds)\n",
    "    \n",
    "    general_loss = F.cross_entropy(preds.contiguous().view(-1, preds.size(-1)),\n",
    "                                   ys, ignore_index=opt.trg_pad).to(opt.device)\n",
    "    total_loss = opt.gen_loss_scale * general_loss + \\\n",
    "                 opt.rhymes_loss_scale * loss_rhymes + \\\n",
    "                 (loss_metre + loss_syll) * 0.001\n",
    "#     total_loss = general_loss + (loss_metre + loss_syll) * 0.001\n",
    "#     total_loss = total_loss * (1 - batch.EB.to(opt.device).mean())\n",
    "    \n",
    "    total_loss.backward()\n",
    "    opt.optimizer.step()\n",
    "    if opt.SGDR == True: \n",
    "        opt.sched.step()\n",
    "    \n",
    "    return total_loss.item(), loss_syll.item(), loss_metre.item(), loss_rhymes.item(), general_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_metre_vocab = len(METRE.vocab.itos)\n",
    "len_xsampa_vocab = len(XSAMPA.vocab.itos)\n",
    "\n",
    "syll_net = nn.Sequential(\n",
    "            nn.Linear(16, opt.SYLL_DIM),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(opt.SYLL_DIM, opt.SYLL_DIM)\n",
    "        ).to(opt.device)\n",
    "\n",
    "emb_metre = nn.Embedding(len_metre_vocab, opt.METRE_EMB_DIM).to(opt.device)\n",
    "\n",
    "emb_char = nn.Embedding(len_xsampa_vocab, opt.CHAR_LSTM_EMB_DIM).to(opt.device)\n",
    "\n",
    "bert.eval()\n",
    "char_lstm = Char_LSTM(opt).to(opt.device)\n",
    "\n",
    "pred_syll_layer = nn.Linear(opt.d_model, opt.SYLL_DIM).to(opt.device)\n",
    "pred_metre_layer = nn.Linear(opt.d_model, opt.METRE_EMB_DIM).to(opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable params: 236145083\n",
      "Total number of params: 413998523\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(opt, len(BERT_TXT.vocab.itos), opt.d_model, opt.n_layers,\n",
    "                 opt.heads, opt.dropout, bert, char_lstm, syll_net, \n",
    "                 emb_metre, emb_char).to(opt.device)\n",
    "\n",
    "for name, param in model.named_parameters(): \n",
    "    if 'bert' in name:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "for name, param in model.named_parameters(): \n",
    "    if param.dim() > 1 and 'bert' not in name:\n",
    "        nn.init.xavier_uniform_(param)\n",
    "\n",
    "print(\"Number of trainable params: {}\".format(\n",
    "    sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "print(\"Total number of params: {}\".format(\n",
    "    sum(p.numel() for p in model.parameters())))\n",
    "\n",
    "opt.train_len = len(train_iterator)\n",
    "\n",
    "opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, \n",
    "                                 betas=(0.9, 0.98), eps=1e-9)#.to(opt.device)\n",
    "\n",
    "if opt.SGDR == True:\n",
    "    opt.sched = CosineWithRestarts(opt.optimizer, T_max=opt.train_len)#.to(opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./log/last_model_verses.pt\", \n",
    "                                           map_location=torch.device(opt.device)))\n",
    "pred_syll_layer.load_state_dict(torch.load(\"./log/pred_syll_layer_verses.pt\", \n",
    "                                           map_location=torch.device(opt.device)))\n",
    "pred_metre_layer.load_state_dict(torch.load(\"./log/pred_metre_layer_verses.pt\", \n",
    "                                           map_location=torch.device(opt.device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~\n",
      "Epoch: 22 | Epoch Time: 35m 26s\n",
      "\tEpoch Loss: 0.279 / 2007.843\n",
      "\n",
      "\tSyll Loss: 0.051\n",
      "\n",
      "\tMetre Loss: 0.063\n",
      "\n",
      "\tRhymes Loss: 0.162\n",
      "\n",
      "\tGeneral Loss: 0.317\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [SEP] -    - -- # - --   : # - -   -- #    -   . # --   -- ! #  - - --  ; # -    - ; # -     -  ! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [SEP] -  -- :  . #   -   - -- . #   - --  # -- -  -- - . #  - -- ---   -- #  -- ----- - #  - --- : -  ! #  -  --  --- . [SEP] -- -  -  - . [SEP] --  --  --- - #  -- -  - #  -- - --- .   ---- - - #  -- -- --- #  -- -  - # -- -- -- - . #  -- -- - #  -- - ---- #  -- ---  #    - -- : # -- -  -  - . # --  --  --- - #  -- -  - #  -- - --- .\n",
      "***\n",
      "[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [SEP] -    - -- # - --   : # - -   -- #    -   . # --   -- ! #  - - --  ; # -    -  # -     -  ! [CLS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [SEP] -  -- :  . #   -   - -- . #   - --  # -- -  -- - . #  - -- ---   -- #  -- ----- - #  - --- : -  ! #  -  --  --- . [CLS] [SEP] -- -  -  - . # --  --  --- - #  -- -  - #  -- - --- . #  ---- - - #  -- -- --- #  -- -  - # -- -- -- - . #  -- -- - #  -- - ---- #  -- ---  #    - -- : # -- -  -  - . # --  --  --- - #  -- -  - #  -- - --- . [CLS]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [01:34,  1.34s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e0b8cc00cf35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         train_loss, loss_syll, loss_metre, loss_rhymes, general_loss = train_batch(\n\u001b[1;32m     20\u001b[0m                                  \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                  pred_syll_layer, pred_metre_layer)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#         break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-d90bc4cd6d8b>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(opt, batch, model, optimizer, pred_syll_layer, pred_metre_layer)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss_syll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_syll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyll_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRU_SYLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss_metre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_metre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_metre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRU_METRE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss_rhymes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_rhymes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     general_loss = F.cross_entropy(preds.contiguous().view(-1, preds.size(-1)),\n",
      "\u001b[0;32m<ipython-input-31-00a81dc23513>\u001b[0m in \u001b[0;36mget_loss_rhymes\u001b[0;34m(batch, preds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_loss_rhymes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#.to(opt.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloss_rhymes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRU_BERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(\"./log/\", exist_ok=True)\n",
    "lst_epoch_loss = []\n",
    "lst_loss_syll_epoch = []\n",
    "lst_loss_metre_epoch = []\n",
    "lst_loss_rhymes_epoch = []\n",
    "lst_general_loss_epoch = []\n",
    "for epoch in range(opt.epochs):\n",
    "    model.train()\n",
    "    pred_syll_layer.train()\n",
    "    pred_metre_layer.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    loss_syll_epoch = 0\n",
    "    loss_metre_epoch = 0\n",
    "    loss_rhymes_epoch = 0\n",
    "    general_loss_epoch = 0\n",
    "    for idx, batch in tqdm(enumerate(train_iterator)):\n",
    "        train_loss, loss_syll, loss_metre, loss_rhymes, general_loss = train_batch(\n",
    "                                 opt, batch, model, opt.optimizer, \n",
    "                                 pred_syll_layer, pred_metre_layer)\n",
    "#         break\n",
    "        epoch_loss += train_loss / len(train_iterator) / opt.BATCH_SIZE\n",
    "        loss_syll_epoch += loss_syll / len(train_iterator) / opt.BATCH_SIZE\n",
    "        loss_metre_epoch += loss_metre / len(train_iterator) / opt.BATCH_SIZE\n",
    "        loss_rhymes_epoch += loss_rhymes / len(train_iterator) / opt.BATCH_SIZE\n",
    "        general_loss_epoch += general_loss / len(train_iterator) / opt.BATCH_SIZE\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    lst_epoch_loss.append(epoch_loss)\n",
    "    lst_loss_syll_epoch.append(loss_syll_epoch)\n",
    "    lst_loss_metre_epoch.append(loss_metre_epoch)\n",
    "    lst_loss_rhymes_epoch.append(loss_rhymes_epoch)\n",
    "    lst_general_loss_epoch.append(general_loss_epoch)\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(\"Epoch loss\"))\n",
    "    plt.plot(lst_epoch_loss)\n",
    "    plt.show()\n",
    "    \n",
    "    print('~'*5);\n",
    "    print(f'Epoch: {epoch} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tEpoch Loss: {epoch_loss:.3f} / {(epoch_loss*len(train_iterator)*opt.BATCH_SIZE):.3f}\\n')\n",
    "    print(f'\\tSyll Loss: {loss_syll_epoch:.3f}\\n')\n",
    "    print(f'\\tMetre Loss: {loss_metre_epoch:.3f}\\n')\n",
    "    print(f'\\tRhymes Loss: {loss_rhymes_epoch:.3f}\\n')\n",
    "    print(f'\\tGeneral Loss: {general_loss_epoch:.3f}\\n')\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    lst = []\n",
    "    for i in preds.contiguous().view(-1, preds.size(-1)).argmax(dim=1):\n",
    "        lst.append(BERT_RU.vocab.itos[i])\n",
    "    print(\" \".join(\" \".join(lst).replace(\" ##\", \"-\").split()[::-1]))\n",
    "    print(\"***\")\n",
    "    lst = []\n",
    "    for i in batch.RU_BERT:\n",
    "        for j in i:\n",
    "            lst.append(BERT_RU.vocab.itos[j])\n",
    "    print(\" \".join(\" \".join(lst).replace(\" ##\", \"-\").split()[::-1]))\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        torch.save(model.state_dict(), './log/last_model_verses.pt')\n",
    "        torch.save(pred_syll_layer.state_dict(), './log/pred_syll_layer_verses.pt')\n",
    "        torch.save(pred_metre_layer.state_dict(), './log/pred_metre_layer_verses.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_syll_layer.eval()\n",
    "pred_metre_layer.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epi_ru = epitran.Epitran('rus-Cyrl')\n",
    "\n",
    "def xsampa_ru(token):\n",
    "    ret = ''.join(epi_ru.xsampa_list(token))\n",
    "    if ret != '':\n",
    "        return ret\n",
    "    else:\n",
    "        return \"_\"\n",
    "    \n",
    "def get_xsampa_by_cur_bert(string):\n",
    "    tok_num = len(string.split(\" \"))\n",
    "    lst = []\n",
    "    for i in range(1, tok_num+1):\n",
    "        cur_text = tokenizer.decode(\n",
    "            tokenizer.convert_tokens_to_ids(\n",
    "                string.split()[max(0,i-50):i]\n",
    "            ), clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        last_token = cur_text.split(\" \")[-1]\n",
    "        lst.append(xsampa_ru(last_token))\n",
    "    if len(lst) != len(string.split(\" \")):\n",
    "        print(string[:50])\n",
    "    return \" \".join(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.max_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:02,  2.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:05,  2.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:08,  2.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "6it [00:11,  2.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "7it [00:14,  2.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'##up'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-e89e4b36d955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist_of_translations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mid_ba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mMETRE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRU_METRE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"translation\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEN_BERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '##up'"
     ]
    }
   ],
   "source": [
    "list_of_translations = []\n",
    "for id_ba, batch in tqdm(enumerate(test_iterator)):\n",
    "    if METRE.vocab.itos[batch.RU_METRE] == \"translation\":\n",
    "        continue\n",
    "    src = batch.EN_BERT.to(opt.device)\n",
    "    eos_tok = BERT_RU.vocab.stoi['[SEP]']\n",
    "    src_mask = (src != BERT_RU.vocab.stoi['[PAD]']).unsqueeze(-2).to(opt.device)\n",
    "    trg_mask = nopeak_mask(1, opt).to(opt.device)\n",
    "\n",
    "    e_output = model.encoder(batch, src_mask).to(opt.device)\n",
    "\n",
    "    syll = pred_syll_layer(e_output.max(dim=1).values).to(opt.device)\n",
    "    metre = pred_metre_layer(e_output.max(dim=1).values).to(opt.device)\n",
    "    \n",
    "    gen_output_bert = torch.tensor([[BERT_RU.vocab.stoi[\"[CLS]\"]]]).to(opt.device) # batch_size, cur_seq_len\n",
    "    gen_output_xsampa = batch.EN_XSAMPA[:, 0].unsqueeze(1).to(opt.device) # batch_size, cur_seq_len, char_emb_dim\n",
    "    \n",
    "    for idx in range(2, opt.max_len):\n",
    "        trg_mask = nopeak_mask(idx - 1, opt).to(opt.device)\n",
    "        out_bert = bert(gen_output_bert)[0] # [batch_size, cur_seq_len, 768]\n",
    "        out_char_lstm = char_lstm(emb_char(gen_output_xsampa)) # [batch_size, cur_seq_len, lstm_hid_dim]\n",
    "        cur_seq_len = out_char_lstm.shape[1]\n",
    "        out_syll = syll.unsqueeze(1).repeat([1, cur_seq_len, 1]) # [batch_size, cur_seq_len, SYLL_DIM]\n",
    "        out_metre = metre.unsqueeze(1).repeat([1, cur_seq_len, 1]) # [batch_size, cur_seq_len, METRE_EMB_DIM]\n",
    "        out = torch.cat([out_bert, out_char_lstm, out_syll, out_metre], dim = 2)\n",
    "        cur_gen = model.out(model.decoder(out, e_output, src_mask, trg_mask))\n",
    "        cur_gen = cur_gen[:,-1,:].unsqueeze(1).to(opt.device)\n",
    "        tok_to_add = cur_gen.argmax().unsqueeze(0).unsqueeze(0).to(opt.device)\n",
    "\n",
    "        if tok_to_add[0][0].item() == BERT_RU.vocab.stoi[BERT_RU.eos_token]:\n",
    "            break\n",
    "        gen_output_bert = torch.cat([gen_output_bert, tok_to_add], dim = 1)\n",
    "        lst = []\n",
    "        for i in gen_output_bert[0]:\n",
    "            lst.append(BERT_RU.vocab.itos[i])\n",
    "        cur_bert_string = \" \".join(lst)\n",
    "        cur_xsampa_string = get_xsampa_by_cur_bert(cur_bert_string)\n",
    "        xsampa_to_add = XSAMPA.process([cur_xsampa_string.split(\" \")])[:, -2].unsqueeze(1).to(opt.device)\n",
    "        gen_output_xsampa = torch.cat([gen_output_xsampa, xsampa_to_add], dim = 1)\n",
    "    \n",
    "    list_cur_translation = []\n",
    "    for i in gen_output_bert[0]:\n",
    "        list_cur_translation.append(BERT_RU.vocab.itos[i])\n",
    "    list_cur_rubert = []\n",
    "    for i in batch.RU_BERT[0]:\n",
    "        list_cur_rubert.append(BERT_RU.vocab.itos[i])\n",
    "    cur_string = \" \".join(list_cur_translation)\n",
    "    cur_rubert_string = \" \".join(list_cur_rubert)\n",
    "    list_of_translations.append([batch.IDX[0].item(), cur_rubert_string, cur_string])\n",
    "    if id_ba >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7998,\n",
       "  '[CLS] .  ## ##  ## ##  ## ## ## #   ## ## ##  ## ## # :  ##  ## ##  ##  ## #   ## ##  -  ## ## #  ## ##  ## ##   # .  ##  ## ## ## #  ## ##  ## ##    ## ## #  ##  ## ## ##   #   ##  ## ##  ## ##  # .  ##  ## ##   #   ##   # :  ##  ## ## #  ##  ## ## #  ## ##     # !  ## ##   ## ## ## ## #  ##   ## ## ##  ## # !  ## ##  ##   ##  ##  ## ## # !  ##  ##   ## [SEP]',\n",
       "  '[CLS]      ##   ## #     ##  ##  ##  ##       ##  ## ##   ## ## ## ##     ##      '],\n",
       " [8087,\n",
       "  '[CLS]  ## ##   ## ##  # - !  ##  -   # .  ## ##  ## ##  ## ## #  ## ##  ##  #   ##  #  ##  ## ##   ## #    -  ## ## #  ## ## ##  # .  ## ##    #  ## ##  ## #  ## ##  ## #  ##  ## ##  ##  #  ##  ## #  ## ##    ##  # .  ##  ## ##  ## ## #  ## ##  ## ##  #  ## ## ##  # \" !  ## ##  ## \" :  ## ##   ## ## ## #  ## ##  ;  ## #  ## ## ## ##   ##  #  ##  ## ## ## ##  #    ## #  ##  ##  #     ##    #  ## ## ##  #     # 1896 [SEP]',\n",
       "  '[CLS]       ##          ##  ##  ##  ## :  ## : ##  ##   ##  ##          ##   '],\n",
       " [7076,\n",
       "  '[CLS] .  ## ##   ##  #   ## ##   # ?  ##    ## ##   ## #  ## ##  ## ## ## ##  # ?  ##  ## ## ##  ## ## ## ##  ## ## #  ## ## ##    ##    # .  ##  ## ## ##  ## ##  #  ##      ## ##  ## #    ## ## ##  ##    # .  ## ##  ## ##   ##  #  ## ##  ## ##  ## ##  #  ##  ## ##   ## #  ## ##  ## ##     ## ## ## #  ## ##   ## ## ## ##  ##   ## ## #  ## ##    ##    ##  #   ##    ## ## ##  #  ## ##     #     ##   #  ## ## ##   ## ## #   ## ##     ## ##  # .    ##  ## ##     #  ## ##  ## ##    #  ##  ## ##   ## ## ## ##  ## # .  ## ## ## ##  \"  ##  \"   ##  #  ## ## ## ##   ##   ## ##  [SEP]',\n",
       "  '[CLS]   ##           ##         ##    ##      ## ##   ##   ##  ## ##  ##  ## '],\n",
       " [7392,\n",
       "  '[CLS] .  ##  ## ##  ##  ## ## # -  ##    ## ## ##  # ;  ##   ##  #  ##  ##  ## ## ## ##   ##  #     ##  # !   ##  # 7 # .  ##   ## ##    #  ##   ##  ## ##  ## #   ## ##   ## # :  ## ## ##  ## ##   ## ##  #  ## ##  ## ##    #   ## ## ## # 6 # .  ##  ## ## ##  ## -  ##  ## ## ## #  ##  ## ## ##  ## ##   # .  ##  ##  ##  #  ## ##  ##  ## ##  ## ##  # ;  ## ##   ## ## ## ##  #  ## ##  ##  # 5 # .  ## ##  ## ##  ##  ##  #  ##  ##  ## ##   ##  ## ##  # !  ## ##   # -  ## ##   ##  ##  # !  ##  ## ##  ## ## ## # !  ## ## ##  # 4 # !  ## ##  ##  ## ##   ## ##  #  ## ##  ##   ##  ##  # ;  ## ##  ## ##  #  ##    ## ##   ##  #  ##  ## ##  ##  ##   #  ##  ## ## ## # 3 # !  ##    ## !  ## ## ## ##   # ?  ## ## ##   ## ##  ## ##  ## #    ##   #      ##   #     ##  #  ##   # 2 # ?   ## ##    ##  ## ##  # ?  ## ##  ##  ##  -  # ?  ## ##  ## ##  ## ## # -  ##  ## ## ##  ##  ##  #    ##  ## ## ##  ## #   ##  ## # 1 [SEP]',\n",
       "  '[CLS]       ##    ##      ## ##    ##  ##   ##  ##    ##    ##       ##    '],\n",
       " [4281,\n",
       "  '[CLS] \" ?  ##  ## ##  \" # -  ##  ## ## ##   #  ##   ## ## ##  # -  ##    ## ## #  ##    # -    ##   #  ##  ## ##  ## ## # -  ##    ## # ?  ## ##    # -  ##  ## ## ## #  ## ## ##   #  ## ##  ## ##  # .    ## ## ## ## #  ##     #  ## ##    ## ## #  ##  ## ##  # .  ##  ## ##  ## #  ## ##  ## ##  ## #   ##  ## ## #  ## ##  ## ## ## ##  #  ## ##   #  ##  ## ##  #  ## ##  ## ##  #  ##  ## ##  [SEP]',\n",
       "  '[CLS]         ##    ##                 ##    ##   ##       ##    ']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
